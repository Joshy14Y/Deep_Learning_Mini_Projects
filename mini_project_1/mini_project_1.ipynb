{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef47b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c06daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Init and prepare dataset.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.prepare_transforms()\n",
    "        self.setup_loader()\n",
    "\n",
    "    def prepare_transforms(self, num_channels=1, size=(256, 256)):\n",
    "        \"\"\"Assign preprocessing transforms.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.uint8, scale=True),\n",
    "            transforms.Grayscale(num_output_channels=num_channels),\n",
    "            transforms.Resize(size=size, antialias=True),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "        self.dataset.transform = transform\n",
    "\n",
    "    def setup_loader(self, batch_size=32):\n",
    "        \"\"\"Create dataloader for preprocessing.\"\"\"\n",
    "        self.loader = DataLoader(\n",
    "            self.dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "        self.dataset_len = len(self.dataset)\n",
    "\n",
    "    def normalize_mean(self, mean):\n",
    "        \"\"\"Normalize mean by dataset length.\"\"\"\n",
    "        return round((mean / self.dataset_len).item(), 4)\n",
    "\n",
    "    def normalize_std(self, var):\n",
    "        \"\"\"Normalize std from variance by dataset length.\"\"\"\n",
    "        return round(torch.sqrt(var / self.dataset_len).item(), 4)\n",
    "\n",
    "    def sum_batch_stats(self):\n",
    "        \"\"\"Accumulate mean and variance over all batches.\"\"\"\n",
    "        mean, var = 0, 0\n",
    "        for images, _ in self.loader:\n",
    "            B, C, _, _ = images.shape\n",
    "            images = images.view(B, C, -1)\n",
    "            mean += images.mean(dim=2).sum(dim=0)\n",
    "            var += images.var(dim=2).sum(dim=0)\n",
    "        return mean, var\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Compute dataset mean and std.\"\"\"\n",
    "        mean, var = self.sum_batch_stats()\n",
    "        normalized_mean = self.normalize_mean(mean)\n",
    "        normalized_std = self.normalize_std(var)\n",
    "        return normalized_mean, normalized_std\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Return dataset mean and std when instance is called.\"\"\"\n",
    "        return self.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8148e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        \"\"\"Init dataset and loaders.\"\"\"\n",
    "        self.root = \"./data\"\n",
    "        self.dataset = datasets.ImageFolder(self.root)\n",
    "        self.preprocessor = ImagePreprocessor(self.dataset)\n",
    "        self.class_names = self.dataset.classes\n",
    "        self.setup_loaders()        \n",
    "\n",
    "    def get_transforms(self, size=(256, 256), num_channels=1, mean=0.1837, std=0.1893):\n",
    "        \"\"\"Return train/eval transforms.\"\"\"\n",
    "        if mean is None or std is None:\n",
    "            mean, std = self.preprocessor()\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToImage(),\n",
    "                transforms.ToDtype(torch.uint8, scale=True),\n",
    "                transforms.Grayscale(num_output_channels=num_channels),\n",
    "                transforms.RandomResizedCrop(size=size, antialias=True),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToDtype(torch.float32, scale=True),\n",
    "                transforms.Normalize(mean=[mean], std=[std]),\n",
    "            ]\n",
    "        )\n",
    "        transform_eval = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToImage(),\n",
    "                transforms.ToDtype(torch.uint8, scale=True),\n",
    "                transforms.Grayscale(num_output_channels=num_channels),\n",
    "                transforms.Resize(size=size, antialias=True),\n",
    "                transforms.ToDtype(torch.float32, scale=True),\n",
    "                transforms.Normalize(mean=[mean], std=[std]),\n",
    "            ]\n",
    "        )\n",
    "        return transform_train, transform_eval\n",
    "\n",
    "    def split_dataset(self, random_state=42):\n",
    "        \"\"\"Split into train/val/test indices.\"\"\"\n",
    "        targets = [y for _, y in self.dataset.samples]\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        train_idx, eval_idx = train_test_split(\n",
    "            indices, test_size=0.3, stratify=targets, random_state=random_state\n",
    "        )\n",
    "        eval_targets = [targets[idx] for idx in eval_idx]\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            eval_idx, test_size=1 / 3, stratify=eval_targets, random_state=random_state\n",
    "        )\n",
    "        return train_idx, val_idx, test_idx\n",
    "\n",
    "    def create_split_dataset(self, indices, transform):\n",
    "        \"\"\"Return subset dataset with transform.\"\"\"\n",
    "        split_dataset = datasets.ImageFolder(self.root, transform=transform)\n",
    "        split_dataset.samples = [self.dataset.samples[i] for i in indices]\n",
    "        split_dataset.targets = [self.dataset.samples[i][1] for i in indices]\n",
    "        return split_dataset\n",
    "\n",
    "    def get_subsets(self):\n",
    "        \"\"\"Return train/val/test subsets.\"\"\"\n",
    "        transform_train, transform_eval = self.get_transforms()\n",
    "        train_idx, val_idx, test_idx = self.split_dataset()\n",
    "        train = self.create_split_dataset(train_idx, transform_train)\n",
    "        val = self.create_split_dataset(val_idx, transform_eval)\n",
    "        test = self.create_split_dataset(test_idx, transform_eval)\n",
    "        return train, val, test\n",
    "\n",
    "    def create_loader(self, split, batch_size=32, shuffle=True):\n",
    "        \"\"\"Return DataLoader for split.\"\"\"\n",
    "        return DataLoader(\n",
    "            split, batch_size=batch_size, shuffle=shuffle, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def setup_loaders(self):\n",
    "        \"\"\"Init train/val/test loaders.\"\"\"\n",
    "        train, val, test = self.get_subsets()\n",
    "        self.train_loader = self.create_loader(train)\n",
    "        self.val_loader = self.create_loader(val)\n",
    "        self.test_loader = self.create_loader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0053d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMetrics:\n",
    "    def __init__(self):\n",
    "        \"\"\"Init metric counters.\"\"\"\n",
    "        self.reset_values()\n",
    "\n",
    "    def reset_values(self):\n",
    "        \"\"\"Reset all metric values.\"\"\"\n",
    "        self.loss = 0\n",
    "        self.acc = 0\n",
    "        self.correct_preds = 0\n",
    "        self.total_samples = 0\n",
    "\n",
    "    def update_loss(self, loss, batch_size):\n",
    "        \"\"\"Add batch loss to total.\"\"\"\n",
    "        self.loss += loss.item() * batch_size\n",
    "        self.total_samples += batch_size\n",
    "\n",
    "    def update_correct_preds(self, outputs, y):\n",
    "        \"\"\"Add correct predictions from batch.\"\"\"\n",
    "        _, preds = outputs.max(1)\n",
    "        self.correct_preds += (preds == y).sum().item()\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"Return average loss and accuracy.\"\"\"\n",
    "        avg_loss = self.loss / self.total_samples\n",
    "        avg_acc = self.correct_preds / self.total_samples\n",
    "        return avg_loss, avg_acc\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Compute metrics when called.\"\"\"\n",
    "        return self.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db6dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCheckpoint:\n",
    "    def __init__(self, model):\n",
    "        \"\"\"Init checkpoint manager.\"\"\"\n",
    "        self.setup_path()\n",
    "        self.model = model\n",
    "        self.best_acc = 0\n",
    "\n",
    "    def setup_path(self):\n",
    "        \"\"\"Ensure checkpoint dir exists.\"\"\"\n",
    "        self.path = \"./checkpoints/best_model.pt\"\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "\n",
    "    def save(self, acc):\n",
    "        \"\"\"Save model if acc improves.\"\"\"\n",
    "        if acc > self.best_acc:\n",
    "            self.best_acc = acc\n",
    "            torch.save(self.model.state_dict(), self.path)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load model weights.\"\"\"\n",
    "        checkpoint = torch.load(self.path)\n",
    "        self.model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4908394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, model, data, load_checkpoint=False):\n",
    "        \"\"\"Init training setup.\"\"\"\n",
    "        self.device = \"cuda\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.epochs = 100\n",
    "        self.data = data\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), 1e-2)\n",
    "        self.metrics = TrainMetrics()\n",
    "        self.setup_checkpoint(load_checkpoint)\n",
    "\n",
    "    def setup_checkpoint(self, load_checkpoint):\n",
    "        \"\"\"Init checkpoint and optionally load weights.\"\"\"\n",
    "        self.checkpoint = TrainCheckpoint(self.model)\n",
    "        if load_checkpoint:\n",
    "            self.checkpoint.load()\n",
    "\n",
    "    def to_device(self, X, y):\n",
    "        \"\"\"Move batch to device.\"\"\"\n",
    "        return X.to(self.device), y.to(self.device)\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Compute outputs and loss.\"\"\"\n",
    "        outputs = self.model(X)\n",
    "        loss = self.criterion(outputs, y)\n",
    "        return outputs, loss\n",
    "\n",
    "    def backward(self, loss):\n",
    "        \"\"\"Run backprop and optimizer step.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_metrics(self, outputs, y, loss):\n",
    "        \"\"\"Update batch metrics.\"\"\"\n",
    "        batch_size = y.size(0)\n",
    "        self.metrics.update_loss(loss, batch_size)\n",
    "        self.metrics.update_correct_preds(outputs, y)\n",
    "\n",
    "    def run_epoch(self, train_mode=True):\n",
    "        \"\"\"Run one training or validation epoch.\"\"\"\n",
    "        loader = self.data.train_loader if train_mode else self.data.val_loader\n",
    "        self.metrics.reset_values()\n",
    "        self.model.train() if train_mode else self.model.eval()\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            for X, y in loader:\n",
    "                X, y = self.to_device(X, y)\n",
    "                outputs, loss = self.forward(X, y)\n",
    "                if train_mode:\n",
    "                    self.backward(loss)\n",
    "                self.update_metrics(outputs, y, loss)\n",
    "        return self.metrics()\n",
    "\n",
    "    def print_metrics(self, epoch, train_metrics, val_metrics):\n",
    "        \"\"\"Print epoch metrics.\"\"\"\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{self.epochs}] \"\n",
    "            f\"train: loss={train_metrics[0]:.4f}, acc={train_metrics[1]:.2%} | \"\n",
    "            f\"val: loss={val_metrics[0]:.4f}, acc={val_metrics[1]:.2%}\"\n",
    "        )\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Train model and validate each epoch.\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_metrics = self.run_epoch(train_mode=True)\n",
    "            val_metrics = self.run_epoch(train_mode=False)\n",
    "            self.print_metrics(epoch, train_metrics, val_metrics)\n",
    "            self.checkpoint.save(val_metrics[1])\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Start training loop.\"\"\"\n",
    "        self.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a9f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool=True):\n",
    "        \"\"\"Init layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a1c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Init layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(1, 8)\n",
    "        self.conv2 = ConvBlock(8, 16)\n",
    "        self.conv3 = ConvBlock(16, 32)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 32 * 32, 128), nn.ReLU(), nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a3a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationMetrics:\n",
    "    def __init__(self, class_names):\n",
    "        \"\"\"Init confusion matrices.\"\"\"\n",
    "        self.device = \"cuda\"\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(class_names)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all to zeros.\"\"\"\n",
    "        shape = (self.num_classes, 2, 2)\n",
    "        self.cms = torch.zeros(shape, dtype=torch.int16, device=self.device)\n",
    "\n",
    "    def binarize(self, cls, preds, labels):\n",
    "        \"\"\"Binarize predictions and labels.\"\"\"\n",
    "        preds_bin = preds == cls\n",
    "        y_bin = labels == cls\n",
    "        return preds_bin, y_bin\n",
    "\n",
    "    def update_class(self, cls, preds, labels):\n",
    "        \"\"\"Update matrix for one class.\"\"\"\n",
    "        tp = (preds & labels).sum()\n",
    "        fp = (preds & ~labels).sum()\n",
    "        tn = (~preds & ~labels).sum()\n",
    "        fn = (~preds & labels).sum()\n",
    "        self.cms[cls] += torch.tensor(\n",
    "            [[tn, fp], [fn, tp]], dtype=torch.int16, device=self.device\n",
    "        )\n",
    "\n",
    "    def update(self, preds, labels):\n",
    "        \"\"\"Update matrices for all classes.\"\"\"\n",
    "        for cls in range(self.num_classes):\n",
    "            preds_bin, y_bin = self.binarize(cls, preds, labels)\n",
    "            self.update_class(cls, preds_bin, y_bin)\n",
    "\n",
    "    def precision(self, cls):\n",
    "        \"\"\"Compute precision for class.\"\"\"\n",
    "        tp = self.cms[cls, 1, 1]\n",
    "        fp = self.cms[cls, 0, 1]\n",
    "        return tp / (tp + fp)\n",
    "\n",
    "    def recall(self, cls):\n",
    "        \"\"\"Compute recall for class.\"\"\"\n",
    "        tp = self.cms[cls, 1, 1]\n",
    "        fn = self.cms[cls, 1, 0]\n",
    "        return tp / (tp + fn)\n",
    "\n",
    "    def f1(self, cls):\n",
    "        \"\"\"Compute F1 for class.\"\"\"\n",
    "        p = self.precision(cls)\n",
    "        r = self.recall(cls)\n",
    "        return 2 * p * r / (p + r)\n",
    "\n",
    "    def accuracy(self, cls):\n",
    "        \"\"\"Compute accuracy for class.\"\"\"\n",
    "        tn = self.cms[cls, 0, 0]\n",
    "        fp = self.cms[cls, 0, 1]\n",
    "        fn = self.cms[cls, 1, 0]\n",
    "        tp = self.cms[cls, 1, 1]\n",
    "        total = tp + tn + fp + fn\n",
    "        return (tp + tn) / total\n",
    "\n",
    "    def print_metrics(self):\n",
    "        \"\"\"Print all metrics per class.\"\"\"\n",
    "        for cls in range(self.num_classes):\n",
    "            name = self.class_names[cls].capitalize()\n",
    "            print(f\"{name}:\")\n",
    "            print(f\"Confusion Matrix:\\n{self.cms[cls]}\")\n",
    "            print(f\"Accuracy: {self.accuracy(cls):.2%}\")\n",
    "            print(f\"Precision: {self.precision(cls):.2%}\")\n",
    "            print(f\"Recall: {self.recall(cls):.2%}\")\n",
    "            print(f\"F1: {self.f1(cls):.2%}\\n\")\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Print metrics when called.\"\"\"\n",
    "        self.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33cffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self, model, data):\n",
    "        \"\"\"Init model, test loader, and metrics.\"\"\"\n",
    "        self.model = model\n",
    "        self.test_loader = data.test_loader\n",
    "        self.device = \"cuda\"\n",
    "        self.cm = ClassificationMetrics(data.class_names)\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Evaluate model on test data.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.test_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                _, preds = outputs.max(1)\n",
    "                self.cm.update(preds, y)\n",
    "        self.cm()\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Run evaluation.\"\"\"\n",
    "        return self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03003d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8f4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "train = Train(model=model, data=data, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248aa1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glioma:\n",
      "Confusion Matrix:\n",
      "tensor([[527,  14],\n",
      "        [  5, 157]], device='cuda:0', dtype=torch.int16)\n",
      "Accuracy: 97.30%\n",
      "Precision: 91.81%\n",
      "Recall: 96.91%\n",
      "F1: 94.29%\n",
      "\n",
      "Healthy:\n",
      "Confusion Matrix:\n",
      "tensor([[500,   3],\n",
      "        [  4, 196]], device='cuda:0', dtype=torch.int16)\n",
      "Accuracy: 99.00%\n",
      "Precision: 98.49%\n",
      "Recall: 98.00%\n",
      "F1: 98.25%\n",
      "\n",
      "Meningioma:\n",
      "Confusion Matrix:\n",
      "tensor([[525,  13],\n",
      "        [ 14, 151]], device='cuda:0', dtype=torch.int16)\n",
      "Accuracy: 96.16%\n",
      "Precision: 92.07%\n",
      "Recall: 91.52%\n",
      "F1: 91.79%\n",
      "\n",
      "Pituitary:\n",
      "Confusion Matrix:\n",
      "tensor([[527,   0],\n",
      "        [  7, 169]], device='cuda:0', dtype=torch.int16)\n",
      "Accuracy: 99.00%\n",
      "Precision: 100.00%\n",
      "Recall: 96.02%\n",
      "F1: 97.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = Test(model=model, data=data)()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
